# ============================================================================
# Browser Agent Configuration - OpenRouter Edition
# ============================================================================

# ===== REQUIRED: OpenRouter API Configuration =====
# Sign up at https://openrouter.ai and get your API key from:
# https://openrouter.ai/keys
#
# FREE TIER MODELS:
# - upstage/solar-pro-3:free (recommended for agents)
# - meta-llama/llama-3.2-3b-instruct:free
# - google/gemma-2-9b-it:free
#
# NOTE: The AsyncOpenAI client is compatible with OpenRouter's API

OPENAI_API_KEY=your_openrouter_api_key_here
API_BASE_URL=https://openrouter.ai/api/v1
MODEL_NAME=upstage/solar-pro-3:free

# ===== OPTIONAL: Network Settings =====
# HTTP Proxy URL (leave empty if not using proxy)
PROXY_URL=

# HTTP request timeout in seconds
# Reduced from 900s (Ollama era) to 120s for cloud APIs
# Increase if you experience timeout errors on slow connections
HTTP_TIMEOUT=120.0

# ===== Browser Settings =====
# Directory for persistent browser session (cookies, localStorage)
USER_DATA_DIR=./browser_data

# Run browser in headless mode (no visible window)
# Set to true for production/servers, false for debugging
HEADLESS=false

# Milliseconds delay between browser actions (anti-fingerprinting)
# Higher values = more human-like, but slower execution
SLOW_MO=50

# Page load timeout in milliseconds
PAGE_LOAD_TIMEOUT=60000

# Individual action timeout in milliseconds (clicks, typing, etc)
ACTION_TIMEOUT=20000

# ===== Agent Behavior =====
# Maximum reasoning-action steps before giving up
# Higher = more persistent, but risks infinite loops
MAX_STEPS=50

# Number of retry attempts for failed actions
MAX_RETRY_ATTEMPTS=3

# LLM temperature (0.0 = deterministic, 2.0 = creative)
# Lower is better for structured agent tasks
TEMPERATURE=0.1

# Maximum tokens in LLM response
# Solar Pro supports up to 4K, but 1K is usually enough for action JSON
MAX_TOKENS=1000

# ===== DOM Processing & Token Optimization =====
# Maximum characters per text block when simplifying DOM
# Higher = more context, but more tokens consumed
TEXT_BLOCK_MAX_LENGTH=500

# Maximum estimated tokens for DOM representation
# Used for context window management
DOM_MAX_TOKENS_ESTIMATE=10000

# ===== Loop Detection =====
# Number of consecutive identical states to check
# Lower = faster detection, but more false positives
LOOP_DETECTION_WINDOW=3

# Maximum identical states before intervention
MAX_IDENTICAL_STATES=5

# ===== Stealth Mode =====
# Enable playwright-stealth to mask automation
# Helps avoid bot detection on some sites
ENABLE_STEALTH=true

# Random typing delays (milliseconds)
# Creates human-like typing patterns
TYPING_SPEED_MIN=50
TYPING_SPEED_MAX=150

# ===== Debugging =====
# Enable debug mode for verbose logging
DEBUG_MODE=false

# Directory for error screenshots and HTML dumps
SCREENSHOT_DIR=./screenshots

# ============================================================================
# MIGRATION NOTES
# ============================================================================
# This configuration is for OpenRouter API.
# If you previously used Ollama, note these changes:
#
# REMOVED:
# - Ollama-specific localhost URLs (http://127.0.0.1:11434)
# - Model names like "llama3:8b" (use OpenRouter format)
# - Ultra-long timeouts for local model generation
#
# ADDED:
# - OpenRouter API authentication
# - Cloud-optimized timeouts
# - Free tier model recommendations
#
# For questions, see: README.md â†’ Troubleshooting section
# ============================================================================
